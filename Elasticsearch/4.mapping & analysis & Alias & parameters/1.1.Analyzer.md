# üìå Elasticsearch Analysis - Complete Guide (Hinglish)

## üîç What is Analysis?

Elasticsearch me **text-based search** ko optimize karne ke liye **Analysis** ka use hota hai. Jab bhi hum koi text field index karte hain, toh uska ek **analysis process** hota hai jo us text ko ek **searchable format** me convert karta hai.

**Analysis ka main goal:**

- Text ko efficiently store karna
- Search queries ko accurately match karna
- Indexing aur retrieval fast karna

---

## ‚öôÔ∏è Process of Analysis

Elasticsearch analysis **3 main parts** me divided hota hai:

| #   | Process Name          | Description                                                                                                |
| --- | --------------------- | ---------------------------------------------------------------------------------------------------------- |
| 1   | **Character Filters** | Input text ko modify karta hai (HTML remove karna, characters replace karna, etc.)                         |
| 2   | **Tokenizer**         | Text ko **tokens (words)** me split karta hai.                                                             |
| 3   | **Token Filters**     | Tokens ko modify ya filter karta hai (lowercase karna, stopwords remove karna, stemming apply karna, etc.) |

---

## üõ†Ô∏è 1. Character Filters (Options & Use Cases)

Character filters text ko modify karte hain **before** tokenizer execute hota hai.

| Character Filter    | Description                                     | Use Case                       |
| ------------------- | ----------------------------------------------- | ------------------------------ |
| **html_strip**      | HTML tags remove karta hai                      | HTML content search ke liye    |
| **mapping**         | Character mapping karta hai (e.g., `&` ‚Üí `and`) | Custom symbol replacement      |
| **pattern_replace** | Regex ke through pattern replace karta hai      | Special character remove karna |

---

## üõ†Ô∏è 2. Tokenizer (Options & Use Cases)

Tokenizer **text ko tokens me todta hai**. Har analyzer me ek tokenizer required hota hai.

| Tokenizer      | Description                                                                    | Use Case               |
| -------------- | ------------------------------------------------------------------------------ | ---------------------- |
| **standard**   | Default tokenizer, Unicode standard ke according split karta hai               | General text search    |
| **whitespace** | Sirf whitespace par split karta hai                                            | Simple keyword search  |
| **keyword**    | Pura text ek single token me store karta hai                                   | Exact phrase search    |
| **letter**     | Sirf letters ko tokenize karta hai, numbers & special characters hata deta hai | Alphanumeric filtering |
| **pattern**    | Regex-based splitting karta hai                                                | Complex text parsing   |

---

## üõ†Ô∏è 3. Token Filters (Options & Use Cases)

Token filters **tokens modify karte hain** after tokenizer execution.

| Token Filter  | Description                                                   | Use Case                |
| ------------- | ------------------------------------------------------------- | ----------------------- |
| **lowercase** | Sab words ko lowercase me convert karta hai                   | Case-insensitive search |
| **stop**      | Common stop words (e.g., 'is', 'the') ko remove karta hai     | Better relevancy        |
| **stemmer**   | Words ko root form me convert karta hai (e.g., running ‚Üí run) | Better search matching  |
| **synonym**   | Synonyms ko replace karta hai (e.g., `fast` ‚Üí `quick`)        | Concept-based search    |

---

## simple example of analyze APIs

```json
POST /_analyze
{
  "text":"<p>The engines are running smoothly, and the lights have been switched on.</p>",
  "analyzer":"standard"
}

POST /_analyze
{
  "text":"<p>The engines are running smoothly, and the lights have been switched on.</p>",
  "char_filter":["html_strip"],
  "tokenizer":"standard",
  "filter":["lowercase","stop","stemmer"]
}
```

---

## üéØ What is Custom Analyzer?

Custom Analyzer ka matlab hota hai ki hum apne **specific use case** ke liye khud ka analysis pipeline define karein **(Character Filter + Tokenizer + Token Filters)**.

‚úÖ **Custom Analyzer me sirf system ke predefined options ka use kar sakte hain.**

## üìù Simple Example of Analysis

Agar hum **custom analyzer** banaye jo **HTML strip kare, whitespace-based tokenization kare aur lowercase conversion kare**, toh hum aise define karenge:

```json
PUT my_index
{
  "settings": {
    "analysis": {
      "char_filter": {
        "my_html_strip": { "type": "html_strip" }
      },
      "tokenizer": {
        "my_whitespace_tokenizer": { "type": "whitespace" }
      },
      "filter": {
        "my_lowercase": { "type": "lowercase" }
      },
      "analyzer": {
        "my_custom_analyzer": {
          "type": "custom",
          "char_filter": ["my_html_strip"],
          "tokenizer": "my_whitespace_tokenizer",
          "filter": ["my_lowercase"]
        }
      }
    }
  }
}

// multiple option example
PUT my_index
{
  "settings": {
    "analysis": {
      "char_filter": {
        "my_html_strip": { "type": "html_strip" },
        "my_mapping_filter": {
          "type": "mapping",
          "mappings": ["phd => doctor"]
        }
      },
      "tokenizer": {
        "my_whitespace_tokenizer": { "type": "whitespace" }
      },
      "filter": {
        "my_lowercase": { "type": "lowercase" },
        "my_stopwords": { "type": "stop", "stopwords": ["is", "the", "a"] }
      },
      "analyzer": {
        "my_custom_analyzer": {
          "type": "custom",
          "char_filter": ["my_html_strip", "my_mapping_filter"],
          "tokenizer": "my_whitespace_tokenizer",
          "filter": ["my_lowercase", "my_stopwords"]
        }
      }
    }
  }
}

```
